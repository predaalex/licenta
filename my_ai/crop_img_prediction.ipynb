{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef2cf12",
   "metadata": {},
   "source": [
    "## Clasificarea imaginilor folosind descriptori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd72e36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.611298500Z",
     "start_time": "2023-05-12T03:52:26.561942800Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from joblib import dump, load\n",
    "from skimage.feature import hog\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911d785",
   "metadata": {},
   "source": [
    "### Incarcarea imaginilor de antrenare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a37cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.612354400Z",
     "start_time": "2023-05-12T03:52:26.579866500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\resources\\\\train\\\\piese_portocalii', '..\\\\resources\\\\train\\\\piese_verzi', '..\\\\resources\\\\train\\\\pozitii_libere']\n"
     ]
    }
   ],
   "source": [
    "path = '..\\\\resources\\\\train\\\\'\n",
    "folders = glob.glob(path + '*')\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b5f814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.612354400Z",
     "start_time": "2023-05-12T03:52:26.596530400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96869efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.861272900Z",
     "start_time": "2023-05-12T03:52:26.610298500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piese_portocalii\n",
      "piese_verzi\n",
      "pozitii_libere\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    \"piese_portocalii\",\n",
    "    \"piese_verzi\",\n",
    "    \"pozitii_libere\"\n",
    "]\n",
    "for clas in folders:\n",
    "    # clas=f.replace(path,'')\n",
    "    print(clas)\n",
    "    files = glob.glob(path + clas + '\\\\*jpg')\n",
    "    for i in files:\n",
    "        image = cv.imread(i)\n",
    "        image = cv.resize(image, (50, 50), interpolation=cv.INTER_LINEAR)\n",
    "        img = np.asarray(image)\n",
    "        train_images.append(img)\n",
    "        train_labels.append(clas)\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c975c8e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.875405300Z",
     "start_time": "2023-05-12T03:52:26.862779600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772, 50, 50, 3)\n",
      "(772,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170268c",
   "metadata": {},
   "source": [
    "### Incarcarea imaginilor de testare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313ce5ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.922352800Z",
     "start_time": "2023-05-12T03:52:26.875405300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\resources\\\\test\\\\piese_portocalii', '..\\\\resources\\\\test\\\\piese_verzi', '..\\\\resources\\\\test\\\\pozitii_libere']\n"
     ]
    }
   ],
   "source": [
    "path = '..\\\\resources\\\\test\\\\'\n",
    "\n",
    "folders = glob.glob(path + '*')\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb913eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.929437600Z",
     "start_time": "2023-05-12T03:52:26.893994500Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207e18e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:26.986273800Z",
     "start_time": "2023-05-12T03:52:26.911006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piese_portocalii\n",
      "piese_verzi\n",
      "pozitii_libere\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    \"piese_portocalii\",\n",
    "    \"piese_verzi\",\n",
    "    \"pozitii_libere\"\n",
    "]\n",
    "for clas in folders:\n",
    "    print(clas)\n",
    "    files = glob.glob(path + clas + '\\\\*jpg')\n",
    "    for i in files:\n",
    "        image = cv.imread(i)\n",
    "        image = cv.resize(image, (50, 50), interpolation=cv.INTER_LINEAR)\n",
    "        img = np.asarray(image)\n",
    "        test_images.append(img)\n",
    "        test_labels.append(clas)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94595a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.046743600Z",
     "start_time": "2023-05-12T03:52:26.987273700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 50, 50, 3)\n",
      "(179,)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4128d",
   "metadata": {},
   "source": [
    "### Diferite tipuri de descriptori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allex\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\allex\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "layer = model._modules.get('avgpool')\n",
    "model.eval()\n",
    "# print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.216746200Z",
     "start_time": "2023-05-12T03:52:27.006260400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_resnet_descriptors(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    folders = [\n",
    "        \"piese_portocalii\",\n",
    "        \"piese_verzi\",\n",
    "        \"pozitii_libere\"\n",
    "    ]\n",
    "    for clasa in folders:\n",
    "        # clas=f.replace(path,'')\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        scaler = transforms.Resize((50, 50))\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        layer = model._modules.get('avgpool')\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        print(clasa + \" \" + path)\n",
    "        files = glob.glob(path + clasa + '\\\\*jpg')\n",
    "        for file in files:\n",
    "            img_cv = cv.imread(file)\n",
    "\n",
    "            img_PIL = Image.fromarray(img_cv)\n",
    "            t_img = Variable(normalize(to_tensor(scaler(img_PIL))).unsqueeze(0))\n",
    "            my_embedding = torch.zeros(512)\n",
    "            def copy_data(m, i, o):\n",
    "                my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
    "            h = layer.register_forward_hook(copy_data)\n",
    "            model(t_img)\n",
    "            h.remove()\n",
    "            my_embedding = my_embedding.numpy()\n",
    "\n",
    "            images.append(my_embedding)\n",
    "            labels.append(clasa)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.262234800Z",
     "start_time": "2023-05-12T03:52:27.219746900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b5b6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.273399100Z",
     "start_time": "2023-05-12T03:52:27.234056300Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_flatten_images(images):\n",
    "    images_flatten = []\n",
    "    for image in images:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "        image = image.flatten()\n",
    "        images_flatten.append(image)\n",
    "    images_flatten = np.array(images_flatten)\n",
    "    return images_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "880e5051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.274400500Z",
     "start_time": "2023-05-12T03:52:27.262234800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gray_histogram(images):\n",
    "    gray_histograms = []\n",
    "    for image in images:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "        hist = cv.calcHist([image], [0], None, [256], [0, 256])\n",
    "        gray_histograms.append(hist.flatten())\n",
    "    gray_histograms = np.array(gray_histograms)\n",
    "    return gray_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "440eb474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.282549100Z",
     "start_time": "2023-05-12T03:52:27.266535500Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_rgb_histogram(images):\n",
    "    color_histograms = []\n",
    "    for img in images:\n",
    "        hist = cv.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        color_histograms.append(hist.flatten())\n",
    "    color_histograms = np.array(color_histograms)\n",
    "    return color_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c34f104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.301191500Z",
     "start_time": "2023-05-12T03:52:27.281427100Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_hog_descriptors(images):\n",
    "    descriptors = []\n",
    "    for img in images:\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        features = hog(img, pixels_per_cell=(8, 8), orientations=9, cells_per_block=(2, 2), block_norm='L2-Hys', feature_vector=True)\n",
    "        descriptors.append(features)\n",
    "    descriptors = np.array(descriptors)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcc185",
   "metadata": {},
   "source": [
    "### Calcularea descriptorilor pentru imaginile de antrenare si testare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6833614c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.311130600Z",
     "start_time": "2023-05-12T03:52:27.297191800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images_flatten = get_flatten_images(train_images)\n",
    "test_images_flatten = get_flatten_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eaae100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.341159600Z",
     "start_time": "2023-05-12T03:52:27.313637400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images_gray_hist = get_gray_histogram(train_images)\n",
    "test_images_gray_hist = get_gray_histogram(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c4dbbc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.357184200Z",
     "start_time": "2023-05-12T03:52:27.342166700Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images_color_hist = get_rgb_histogram(train_images)\n",
    "test_images_color_hist = get_rgb_histogram(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9609822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:27.746691600Z",
     "start_time": "2023-05-12T03:52:27.357184200Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images_hog = get_hog_descriptors(train_images)\n",
    "test_images_hog = get_hog_descriptors(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piese_portocalii ..\\resources\\train\\\n",
      "piese_verzi ..\\resources\\train\\\n",
      "pozitii_libere ..\\resources\\train\\\n",
      "piese_portocalii ..\\resources\\test\\\n",
      "piese_verzi ..\\resources\\test\\\n",
      "pozitii_libere ..\\resources\\test\\\n"
     ]
    }
   ],
   "source": [
    "path = '..\\\\resources\\\\train\\\\'\n",
    "train_images_resnet, train_labels_resnet = get_resnet_descriptors(path)\n",
    "\n",
    "path = '..\\\\resources\\\\test\\\\'\n",
    "test_images_resnet, test_labels_resnet = get_resnet_descriptors(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:35.088737600Z",
     "start_time": "2023-05-12T03:52:27.748691900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d1e36d83",
   "metadata": {},
   "source": [
    "### Antrenarea unui SVM liniar cu diferiti descriptori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piese_portocalii' 'piese_portocalii' 'piese_portocalii']\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:35.104799700Z",
     "start_time": "2023-05-12T03:52:35.085737600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61929e7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:35.286457200Z",
     "start_time": "2023-05-12T03:52:35.101797200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antrenam un clasificator pentru c=0.000010\n",
      "0.9974093264248705\n",
      "Antrenam un clasificator pentru c=0.000100\n",
      "1.0\n",
      "Antrenam un clasificator pentru c=0.001000\n",
      "1.0\n",
      "Antrenam un clasificator pentru c=0.010000\n",
      "1.0\n",
      "Antrenam un clasificator pentru c=0.100000\n",
      "1.0\n",
      "Antrenam un clasificator pentru c=1.000000\n",
      "1.0\n",
      "Performanta clasificatorului optim pt c = 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allex\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "LinearSVC(C=0.0001)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.0001)</pre></div></div></div></div></div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_c = 0\n",
    "best_model = None\n",
    "Cs = [10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1, 10 ** 0]\n",
    "for c in Cs:\n",
    "    print('Antrenam un clasificator pentru c=%f' % c)\n",
    "    model = LinearSVC(C=c)\n",
    "    model.fit(train_images_resnet, train_labels_resnet)\n",
    "    acc = model.score(train_images_resnet, train_labels_resnet)\n",
    "    print(acc)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_c = c\n",
    "        best_model = deepcopy(model)\n",
    "\n",
    "print('Performanta clasificatorului optim pt c = %f' % best_c)\n",
    "\n",
    "# salvam cel mai bun model\n",
    "dump(best_model,\"model_clasificare_piesa.joblib\")\n",
    "\n",
    "# incarcam modelul\n",
    "load(\"model_clasificare_piesa.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e02def9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T03:52:35.363471700Z",
     "start_time": "2023-05-12T03:52:35.288457200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = best_model.predict(test_images_resnet)\n",
    "# print(predicted_labels)\n",
    "print(len(predicted_labels))\n",
    "print(accuracy_score(test_labels_resnet, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
